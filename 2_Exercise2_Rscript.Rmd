---
title: "Exercise 1"
output: 
  html_document: 
    number_sections: true
    toc: true
---

# Exercise 2

# Goals

-   Go big with a dataset consisting of multiple samples:

    -   Assess the quality of your data, apply QC and pre-process

    -   Explore and structure the data

    -   Automated annotation

-   Combine datasets

    -   Batch correction

# TODO

Batch effect (include sample from another dataset?) -\> STACAS tutorial to measure batch effect/mixing after integration

Ex1 -\> manual annotation -\> compare to provided annotation -\> calculate accuracy

# Introduction

In the second exercise, you will apply what you learnt in exercise 1 to annotate a breast cancer dataset that you started exploring in David Gfeller's part of the course. This time, however, you can use the packages for fast processing. The focus is less on the technical (coding/maths) aspect but more on getting a feeling for the biological aspects of the data you are working with.

Let's face it: biological samples, especially **human samples, can have big variances** that we cannot always control for (or in other words: expect a dirty mess).

-   **Experimental (wet lab):**

    -   Mistakes

    -   Contaminations

    -   Sampling differences (by taking a small part of a tissue it might by chance contain more of one cell type e.g.: more blood, or tumor, or healthy, or immune cells)

    -   Processing (e.g. cells might be influenced by how long they are digested or left standing around, even the weather on one day might have exposed some of your samples to more sunlight than other ones), ...

-   **Technical:**

    -   Sequencing technology

    -   Batch effects

-   **Humans â‰  cloned, environmentally-controlled-grown lab mice:**\
    Consider that each human sample is based on a **different genetic and environmental background**. Some might be young/old, adipose/underweight, healthy/unhealthy diet, sportive/sedentary, smoking/non-smoking, "healthy"/additional disease/medication, different treatment history, etc. Sometimes, you get additional metadata for your sample data, which can help explain some of the variance ... or it might not. Especially at (typically) low sample numbers, the effect size might not be significant. In the worst case, it might introduce a new confounder as it seems to correlate with some observed effects, simply by chance.

Unfortunately, there is no optimal solution to account for all of the above confounders (except more data). You have to consider each of them on a case-by-case basis. However, some of them can be controlled for quite well, for example:

-   Experimental: by basic QC metrics and "obvious" outlier detection (of course only if you have a very good explanation to justify it) (of course [we must not select our data to make it comply with a hypothesis!]{.underline})

-   Technical: batch correction by [Harmony](https://www.nature.com/articles/s41592-019-0619-0) (popular but in all honesty not very good) or [STACAS](https://academic.oup.com/bioinformatics/article/37/6/882/5897412) (see [benchmark](https://www.biorxiv.org/content/10.1101/2023.07.07.548105v1) comparing different batch correction tools)

Let's dive in and have a look at the real-world data! :)

```{r, message=FALSE, warning=F, results=FALSE}

# Load the environment and libraries
renv::restore() # Run once, when you download a project folder and want to install all dependencies.
library(Seurat)
library(ggplot2)
library(dplyr)
```

# Load the data

```{r, message=FALSE, warning=F}
# Download the compressed .zip file (too large for GitHub)
zipfile = "./data/exercise_2.zip"
unzip_path = "./data/exercise_2/"

if(!dir.exists(unzip_path)){
  download.file(url = "https://www.dropbox.com/scl/fi/sqp3x7pdfcam9iibypsv5/exercise_2.zip?rlkey=8kzct7u49vjlba815f5agm20z&dl=1",
              destfile = zipfile)
  unzip(zipfile = zipfile, exdir = unzip_path)
}

# Load the dataset
brca.data = ReadMtx(paste0(unzip_path, "matrix.mtx"),
                    paste0(unzip_path, "barcodes.tsv"),
                    paste0(unzip_path, "genes.tsv"))

# Initialize the Seurat object with the raw (non-normalized) data.
obj <- CreateSeuratObject(counts = brca.data, project = "brca",
                          min.cells = 3, min.features = 200)
obj

rm(brca.data)
```

# QC and pre-processing

## Quality control (QC)

```{r paged.print=FALSE}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
obj[["percent.mt"]] <- PercentageFeatureSet(obj, pattern = "^MT-")

# Show QC metrics for the first 5 cells
head(obj@meta.data, 10)
```

We visualize QC metrics, and use these to filter cells.

```{r, fig.width=10}
# Visualize QC metrics as a violin plot
VlnPlot(obj, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
VlnPlot(obj, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3, pt.size = 0)

# FeatureScatter is typically used to visualize feature-feature relationships, but can be used for anything calculated by the object, i.e. columns in object metadata, PC scores etc.
# Compute the ratio of number of genes/features and number of counts/UMIs
plot1 <- FeatureScatter(obj, feature1 = "nCount_RNA", feature2 = "percent.mt") & NoLegend()
plot2 <- FeatureScatter(obj, feature1 = "nCount_RNA", feature2 = "nFeature_RNA") & NoLegend()

plot1 + plot2
rm(plot1, plot2)
```

-   What do you notice?

```{r warning=FALSE, fig.width=10}
sample.size.before <- sort(table(obj$orig.ident))
sample.size.before
barplot(sort(table(obj$orig.ident)), las=2)

nFeature_RNA_maxthreshold <- 2500
mito_threshold_perc <- 5

# Seurat plots can be easily modified with the popular ggplots2 library by chaining it with the "&" operator
plot1 <- VlnPlot(obj, features = "nFeature_RNA", pt.size = 0) &
  geom_hline(yintercept = c(200, nFeature_RNA_maxthreshold), linetype='dashed', col = 'red') & NoLegend()
plot2 <- VlnPlot(obj, features = "nCount_RNA", pt.size = 0) & NoLegend()
plot3 <- VlnPlot(obj, features = "percent.mt", pt.size = 0) &
  geom_hline(yintercept = mito_threshold_perc, linetype='dashed', col = 'red') & NoLegend()
patchwork::wrap_plots(plot1, plot2, plot3, ncol = 3)
rm(plot1, plot2, plot3)

# Subset data according to our filter creteria
subset_test <- subset(obj,
                      subset = nFeature_RNA > 200 & nFeature_RNA < nFeature_RNA_maxthreshold &
                        percent.mt < mito_threshold_perc)

# FeatureScatter is typically used to visualize feature-feature relationships, but can be used for anything calculated by the object, i.e. columns in object metadata, PC scores etc.
plot1 <- FeatureScatter(subset_test,
                        feature1 = "nCount_RNA",
                        feature2 = "percent.mt") & NoLegend()
plot2 <- FeatureScatter(subset_test,
                        feature1 = "nCount_RNA",
                        feature2 = "nFeature_RNA") & NoLegend()
plot1 + plot2
rm(plot1, plot2)

sample.size.after <- sort(table(subset_test$orig.ident))
sample.size.after
barplot(sort(table(subset_test$orig.ident)), las=2)

barplot(sort(sample.size.after/sample.size.before), las=2)
```

-   Do you think these are appropriate filter criteria for percent.mt? Why?\
    -\> Choose a better threshold

-   Do you think these are appropriate filter criteria for nFeature_RNA? Why?\
    -\> Choose a better threshold

-   Do you think it makes sense to include/exclude samples with very low cell numbers? Why yes? Why not?\
    Do you think something might be fishy about them?\
    What situations/reasons when it makes sense to exclude certain samples?

### Apply the final threshold filter

```{r warning=FALSE, fig.width=10}
nFeature_RNA_maxthreshold <- 4500
mito_threshold_perc <- 12

# Subset data according to the filter criteria
obj <- subset(obj,
              subset = nFeature_RNA > 200 & nFeature_RNA < nFeature_RNA_maxthreshold &
                percent.mt < mito_threshold_perc)

```

## Pre-processing

```{r, warning=FALSE}
# Normalize data
obj <- NormalizeData(obj, normalization.method = "LogNormalize", scale.factor = 10000)

# Find variable features
obj <- FindVariableFeatures(obj, selection.method = "vst", nfeatures = 500)
top10 <- head(VariableFeatures(obj), 10)
plot1 <- VariableFeaturePlot(obj)
plot1 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1
rm(top10, plot1)

# Scale data
obj <- ScaleData(obj)
```

-   Do you notice something in the name of the top most variable genes?

-   What are these genes? Can you guess what physiological process is involved, given that we look at breast cancer (disease)?

# Data exploration & visualization

## Performing linear dimensionality reduction by PCA

```{r warning=FALSE, fig.height=7}
obj <- RunPCA(obj, features = VariableFeatures(object = obj))

# Examine and visualize PCA results a few different ways
print(obj[["pca"]], dims = 1:5, nfeatures = 5)
VizDimLoadings(obj, dims = 1:2, reduction = "pca")
DimPlot(obj, reduction = "pca")
DimHeatmap(obj, dims = 1, cells = 500, balanced = TRUE)
DimHeatmap(obj, dims = 2, cells = 500, balanced = TRUE)
```

-   Depending on the parameters you chose (number of variable genes, number of dimensions) some of the following genes might pop up to be among the most important features in PC1 and PC2, explaining the largest part of variance in your data:

    -   COL1A1, COL1A2, COL8A1, COL10A1 and other collagens

    -   CD14, CD68, CD163, C1QA, C1QB, C1QC

    -   CD79A, MS4A1

    -   CXCL10, CCL8, CXCL8, CCL3, CCL2, IL1RN, IL1B

-   Which cell types are associated with the above genes?

-   Does each PCA explain one cell type? Is it obvious what a specific PCA dimension represents?

## Determine the dimensionality of the data

How many dimensions would you use? Are 10 enough?

-   Try to change the number of variable genes or number of dimensions. How does this affect the data?

```{r}
ElbowPlot(obj, ndims=50)
```

TIP: from our experience, it is better to test a few values and [**include rather more than less PCs**]{.underline}. This depends on your data. Including too few principal components might result in missing out on a low abundant cell population. Using excessively many dimensions on the other hand might result in including unwanted noise which does not explain any meaningful variance.

## Clustering

Next, we perform K-nearest neighbor (KNN) clustering, using the previously determined dimensionality of the dataset (e.g. first 30 PCs).

Do you think PCA is a good visualization method for scRNA-seq data as compared UMAP?

Which do you think is better? Why?

```{r warning=FALSE}
obj <- FindNeighbors(obj, dims = 1:30)
obj <- FindClusters(obj, resolution = 0.5)

# Look at cluster IDs of the first 5 cells
head(Idents(obj), 5)

# For comparison, this is how the cluters look in the linear PCA space
DimPlot(obj, reduction = "pca", label = TRUE)
```

## Run non-linear dimensional reduction (UMAP)

```{r warning=FALSE}
obj <- RunUMAP(obj, dims = 1:3)
DimPlot(obj, reduction = "umap", label = TRUE)
obj <- RunUMAP(obj, dims = 1:10)
DimPlot(obj, reduction = "umap", label = TRUE)
obj <- RunUMAP(obj, dims = 1:30)
DimPlot(obj, reduction = "umap", label = TRUE)
obj <- RunUMAP(obj, dims = 1:50)
DimPlot(obj, reduction = "umap", label = TRUE)
```

You can save the object at this point so that it can easily be loaded back in without having to rerun the computationally intensive steps performed above, or easily shared with collaborators.

```{r}
saveRDS(obj, file = "./output/obj_exercise_2.rds")
```

## Finding differentially expressed features (cluster biomarkers)

Seurat can help you find markers that define clusters via differential expression. By default, it identifies positive and negative markers of a single cluster (specified in ident.1), compared to all other cells. FindAllMarkers() automates this process for all clusters, but you can also test groups of clusters vs. each other, or against all cells.

The min.pct argument requires a feature to be detected at a minimum percentage in either of the two groups of cells, and the thresh.test argument requires a feature to be differentially expressed (on average) by some amount between the two groups. You can set both of these to 0, but with a dramatic increase in time - since this will test a large number of features that are unlikely to be highly discriminatory. As another option to speed up these computations, max.cells.per.ident can be set. This will downsample each identity class to have no more cells than whatever this is set to. While there is generally going to be a loss in power, the speed increases can be significant and the most highly differentially expressed features will likely still rise to the top.

```{r paged.print=FALSE}
# find all markers of cluster 2
cluster2.markers <- FindMarkers(obj, ident.1 = 2, min.pct = 0.25)
head(cluster2.markers, n = 5)
```

```{r paged.print=FALSE}
# find all markers distinguishing cluster 5 from clusters 0 and 3
cluster5.markers <- FindMarkers(obj, ident.1 = 5, ident.2 = c(0, 3), min.pct = 0.25)
head(cluster5.markers, n = 5)
```

```{r fig.height=8}
VlnPlot(obj, features = c("MS4A1", "CD79A", "NKG7", "CD8A", "CD8B"), ncol = 2)
```

```{r fig.width=10, fig.height=8}
FeaturePlot(obj, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", "CD8A"))
```

DoHeatmap() generates an expression heatmap for given cells and features. In this case, we are plotting the top 20 markers (or all markers if less than 20) for each cluster.

```{r}
# find markers for every cluster compared to all remaining cells, for example only the positive ones
obj.markers <- FindAllMarkers(obj, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
obj.markers %>%
    group_by(cluster) %>%
    slice_max(n = 2, order_by = avg_log2FC)
```

```{r fig.width=7, fig.height=10}
obj.markers %>%
    group_by(cluster) %>%
    top_n(n = 10, wt = avg_log2FC) -> top10
DoHeatmap(obj, features = top10$gene) + NoLegend()
```

```{r "Load solutions (SPOILER ALERT)"}
solutions_df <- readRDS('./output/1_Solutions.rds')
solutions_df
```

```{r "Apply and plot solutions (SPOILER ALERT)"}
new.cluster.ids <- solutions_df$Cell.Type
names(new.cluster.ids) <- levels(obj)
obj <- RenameIdents(obj, new.cluster.ids)
DimPlot(obj, reduction = "umap", label = TRUE, pt.size = 0.5) + NoLegend()
```
